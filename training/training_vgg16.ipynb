{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5f0b5b2-4e8f-4e82-ab64-1ac96aaa6309",
   "metadata": {},
   "outputs": [],
   "source": [
    "#---------------------------------------------------------------------------------\n",
    "# CREATED BY: Arka Bhowmik and Sarah Eskreis-Winkler, Memorial Sloan Kettering Cancer Center, NY (2022)\n",
    "#\n",
    "# --------------------------------------------------------------------------------\n",
    "# THIS IS A MAIN PROGRAM TO PERFORMS TRAINING OF CLASSIFICATION ALGORITHM (VGG-16) \n",
    "#\n",
    "# \n",
    "# THE MODELS INCLUDES VARIOUS SUB-FUNCTIONS\n",
    "# INSTRUCTIONS: ENSURE ALL SUBFUNCTIONS AND MAIN PROGRAM IN THE SAME FOLDER \n",
    "#               OR CHANGE THE PATH INSIDE ACCORDINGLY\n",
    "#---------------------------------------------------------------------------------\n",
    "# IMPORT IMPORTANT TENSORFLOW AND KERAS LIBRARIES TO RUN DEEP CNN\n",
    "#\n",
    "import tensorflow as tf\n",
    "import tensorflow_addons as tfa\n",
    "#\n",
    "# SUPPORTING LIBRARIES\n",
    "import os\n",
    "import os.path\n",
    "import sys\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import tqdm\n",
    "#\n",
    "# STOPS WARNING AND CHECKS FOR TENSORFLOW VERSION\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)\n",
    "#\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28ff2200-d715-43b2-b72f-f9fc69761b2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "#******************************************************************\n",
    "#                  STEP 1: USER MODEL INPUTS\n",
    "#******************************************************************\n",
    "# Add path for suppoting functions\n",
    "sys.path.append('/mri_triage_normal/support_function/')\n",
    "#\n",
    "# CHANGE ALL INPUT PARAMETERS ONLY IN config_vgg.PY (can be found in support_func)\n",
    "import config_vgg as config\n",
    "#\n",
    "from read_and_split_vgg import read_and_split           # Calls the function for spliting the dataset into training and valid set\n",
    "from user_input_balancing import user_input_balancing   # Calls the function for class balancing (manual oversampling)\n",
    "from create_dictionary_vgg import create_dictionary\n",
    "from Data_gen_vgg import DataGenerator                  # Calls the function for datageneration in case of full image\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5313fb6-4798-4323-9e1f-333cf37e5cc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "#-----------------------------------------------------------------------------\n",
    "#                  STEP 2: DATA PRE-PROCESSING\n",
    "#-----------------------------------------------------------------------------\n",
    "#\n",
    "# THE DATA PRE-PROCESSING STEP READ AND SPLIT DATA INTO TRAIN AND VALID SETS\n",
    "# (IF TRAIN AND VALID ALREADY EXISTS IN THE PATH--> Put split_flg='nosplit'\n",
    "# \n",
    "#******************************************************************************\n",
    "# USER INSTRUCTION:- THIS STEP WILL REPLACE THE EARLIER TRAIN AND VALID FILES\n",
    "# (If split_flg = 'split')\n",
    "#******************************************************************************\n",
    "#\n",
    "counter_tr, counter_vl = read_and_split(config.BASE_PATH,config.CSV_NOTSPLIT,config.split_type,\n",
    "                                        config.split_ID,config.train_ratio,config.validation_ratio,\n",
    "                                        config.split_flg)\n",
    "#\n",
    "#\n",
    "#--------------------------------------------------------------------------------\n",
    "# FUNCTION FOR RANDOM OVERSAMPLING OF WEAKER POPULATION (IF UNBALANCED)\n",
    "#--------------------------------------------------------------------------------\n",
    "#\n",
    "user_input_balancing(counter_tr, counter_vl)\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e894c60-41e1-4c90-ab1e-ba7cd47478c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "#-----------------------------------------------------------------------------\n",
    "#                          STEP 3: DATA GENERATION\n",
    "#-------------------------------------------------------------------------------\n",
    "#\n",
    "# THIS IS A CUSTOM DATA GENERATION STEP\n",
    "# \n",
    "params = {\n",
    "    'batch_size': config.BATCH_SIZE,                  # DEFINE BATCH SIZE\n",
    "    'dim': (config.IMAGE_SIZE, config.IMAGE_SIZE),    # DEFINE IMAGE WIDTH AND HEIGHT\n",
    "    'n_channels': config.IMAGE_CHANNELS,              # DEFINE THE INPUT CHANNELS TO THE NETWORK\n",
    "    'n_classes': config.CLASS_NUM,                    # NUMBER OF CLASSES --> (2)\n",
    "    'shuffle': config.SHUFF,                          # SHUFFLE DATASET EACH EPOCH\n",
    "    'augmentation':config.AGUMENT_METH,               # DATA AGUMENTATION TYPE\n",
    "    'imgsize':config.IMAGE_SIZE}                      # IMAGE SIZE\n",
    "#\n",
    "#---------------------------------------------------------------\n",
    "# CREATING DICTIONARY FILES FOR TRAINING AND VALIDATION SET\n",
    "#---------------------------------------------------------------\n",
    "#\n",
    "partition_tr, labels_tr, impath_tr = create_dictionary(config.train_filename, config.BASE_PATH, 'random')\n",
    "partition_vl, labels_vl, impath_vl = create_dictionary(config.valid_filename, config.BASE_PATH, 'random') \n",
    "#\n",
    "#-------------------------------------------------------------------------------\n",
    "# DATA GENERATORS FOR TRAINING AND VALIDATION SETS\n",
    "#-------------------------------------------------------------------------------\n",
    "training_generator = DataGenerator(partition_tr, labels_tr, impath_tr, **params)\n",
    "validation_generator = DataGenerator(partition_vl, labels_vl, impath_vl, **params)\n",
    "#\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cca468bd-d0cc-4951-8f75-a55722fcef5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "#-----------------------------------------------------------------------------\n",
    "#                          STEP 3A: DISPLAY DATA GENERATION FILE\n",
    "#-------------------------------------------------------------------------------\n",
    "# Visualize the train/valid datagenrator images \n",
    "#\n",
    "plt.figure(figsize=(15, 15));\n",
    "idx=0\n",
    "for X, Y in validation_generator:\n",
    "    for i in range(9): # range(9) only work since batch size is 10\n",
    "        plt.subplot(3,3,i+1)\n",
    "        plt.xticks([])\n",
    "        plt.yticks([])\n",
    "        plt.grid(False)\n",
    "        plt.imshow((X[i]), cmap='gray')\n",
    "        #\n",
    "    #\n",
    "    idx=idx+1\n",
    "    #\n",
    "    if idx > 9:\n",
    "        break\n",
    "    #\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70dca0a3-a7d0-461f-aa7c-602305ca0a52",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "#******************************************************************\n",
    "#          STEP 4: CREATE A CLASSIFICATION CNN MODEL\n",
    "#******************************************************************\n",
    "#\n",
    "base_model = tf.keras.applications.vgg16.VGG16(weights = \"imagenet\", include_top=False, \n",
    "                                               input_shape = (config.IMAGE_SIZE, config.IMAGE_SIZE, \n",
    "                                                              config.IMAGE_CHANNELS), classes = config.CLASS_NUM)\n",
    "# \n",
    "# FROZEN LAYER NOT TRAINABLE\n",
    "for layer in base_model.layers[:config.FROZEN_LAYERS]:\n",
    "    layer.trainable = False\n",
    "#\n",
    "# Flatten the output\n",
    "x = base_model.output\n",
    "flatten = tf.keras.layers.Flatten()(x)\n",
    "# \n",
    "softmaxHead = tf.keras.layers.Dense(512, activation=\"relu\", activity_regularizer=tf.keras.regularizers.l1_l2(l1=1e-5, l2=1e-5))(flatten)\n",
    "softmaxHead = tf.keras.layers.Dropout(0.5)(softmaxHead)\n",
    "softmaxHead = tf.keras.layers.Dense(512, activation=\"relu\", activity_regularizer=tf.keras.regularizers.l1_l2(l1=1e-5, l2=1e-5))(softmaxHead)\n",
    "softmaxHead = tf.keras.layers.Dropout(0.5)(softmaxHead)\n",
    "# \n",
    "output_layer = tf.keras.layers.Dense(2, activation=\"softmax\", activity_regularizer=tf.keras.regularizers.l1_l2(l1=1e-5, l2=1e-5))(softmaxHead)\n",
    "#\n",
    "# Construct the model\n",
    "model = tf.keras.Model(inputs=[base_model.input], outputs =[output_layer])\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8179a4f-2911-4f38-a5d8-05ecb7867192",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Displays the model\n",
    "#\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f3d02d5-a2fb-4af4-833c-1bb05112ca44",
   "metadata": {},
   "outputs": [],
   "source": [
    "#***********************************************************************\n",
    "#           STEP 5: COMPILE THE MODEL AND SHOW SUMMARY\n",
    "#***********************************************************************\n",
    "#\n",
    "opt = tf.keras.optimizers.Adam(learning_rate=config.INIT_LR)        # Adam optimizer\n",
    "model.compile(loss=\"binary_crossentropy\", optimizer=opt, metrics=[\"accuracy\", tf.keras.metrics.AUC()])\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32638f97-23af-4011-996e-3654951f03c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "#----------------------------------------------------------------------------------------\n",
    "#                         STEP 6: TRAINING\n",
    "#----------------------------------------------------------------------------------------\n",
    "# USER DEFINED CALL BACK AND CHECK POINT FUNCTIONS\n",
    "# 1. CALL BACK: STOPS IF VAL_LOSS DOES NOT MINIMIZES FOR 7 CONSECUTIVE EPOOCHS\n",
    "# 2. CHECKPOINT: SAVES THE BEST MODEL WITH MAXIMUM VALIDATION ACCURACY\n",
    "# 3. CALL BACK OPTIONS: REDUCE THE LEARNING RATE IF VALID_LOSS DID NOT REDUCE 7 CONSEQUTIVE EPOOCHS\n",
    "checkpoint_filepath = 'vgg_models/fold1/'+ 'model_' + config.h5file_name + '_{epoch:02d}_acc_{val_accuracy:.3f}_loss_{val_loss:.3f}_AUC_{val_auc:.3f}.h5'\n",
    "#\n",
    "my_callbacks = [\n",
    "    tfa.callbacks.TQDMProgressBar(),\n",
    "    tf.keras.callbacks.EarlyStopping(monitor='val_loss', mode='min', patience=7, verbose=1, restore_best_weights=False),\n",
    "    tf.keras.callbacks.ModelCheckpoint(filepath=os.path.join(config.BASE_OUTPUT, checkpoint_filepath), \n",
    "                                       monitor='val_loss', verbose=1, mode='min', \n",
    "                                       save_best_only=True, save_freq=\"epoch\")]\n",
    "#\n",
    "start = time.time()\n",
    "#\n",
    "train_history = model.fit(training_generator, validation_data=validation_generator,\n",
    "                          epochs = config.NUM_EPOCHS, callbacks=[my_callbacks],\n",
    "                          verbose = 0, initial_epoch=0, workers=4, use_multiprocessing=True)\n",
    "#\n",
    "stop = time.time()\n",
    "#\n",
    "print(f\"Training time: {stop - start}s\")\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e7975d6-9f30-4bfe-a809-a3bf7f9f9e87",
   "metadata": {},
   "outputs": [],
   "source": [
    "#-------------------------------------------------------------------------------------------\n",
    "#\n",
    "#                  STEP 7: STORING THE TRAINING DATA\n",
    "#-------------------------------------------------------------------------------------------\n",
    "# STORES THE DETAILS IN A FILE\n",
    "my_acc_train = train_history.history['accuracy']  # Stores the training accuracy\n",
    "my_loss_train = train_history.history['loss']     # Stores the training loss\n",
    "my_acc_valid = train_history.history['val_accuracy']   # Stores the validation accuracy\n",
    "my_loss_valid = train_history.history['val_loss']      # Stores the validation loss\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "054ab0e1-6a11-4fbd-92a3-9d12f558146e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#------------------------------------------------------------------------------------\n",
    "#                  STEP 8: PLOT THE ACCURACY AND LOSS WITH EPOCH\n",
    "#-----------------------------------------------------------------------------------\n",
    "#\n",
    "if config.plot_ACC_LOSS=='Y':\n",
    "    #------------------------------------------------------------------------------------------\n",
    "    #                         STEP 7: PLOTTING STEPS\n",
    "    #                         (THIS STEP IS OPTIONAL)\n",
    "    fig_name='PLOT_ACC_LOSS_' + config.h5file_name + '.png' \n",
    "    #\n",
    "    def plot_acc_loss(train_history, epochs):\n",
    "        acc = train_history.history['accuracy']\n",
    "        loss = train_history.history['loss']\n",
    "        val_acc = train_history.history['val_accuracy']\n",
    "        val_loss = train_history.history['val_loss']\n",
    "        plt.figure(figsize=(15, 5))\n",
    "        plt.subplot(121)\n",
    "        plt.plot(range(1,epochs), acc[1:], label='Train_acc')\n",
    "        plt.plot(range(1,epochs), val_acc[1:], label='Val_acc')\n",
    "        plt.title('Accuracy over ' + str(epochs) + ' Epochs', size=15)\n",
    "        plt.xlabel('Epochs', size = 14)\n",
    "        plt.ylabel('Accuracy', size = 14)\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        plt.subplot(122)\n",
    "        plt.plot(range(1,epochs), loss[1:], label='Train_loss')\n",
    "        plt.plot(range(1,epochs), val_loss[1:], label='Val_loss')\n",
    "        plt.title('Loss over ' + str(epochs) + ' Epochs', size=15)\n",
    "        plt.xlabel('Epochs', size = 14)\n",
    "        plt.ylabel('Loss', size = 14)\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        plt.savefig(os.path.join(config.BASE_OUTPUT, fig_name))\n",
    "        return()\n",
    "    # CALL THE PLOT FUNCTION\n",
    "    plot_acc_loss(train_history, config.NUM_EPOCHS)  # Specify the number of epoch (since training stopped earlier)\n",
    "    #\n",
    "elif config.plot_ACC_LOSS=='N':\n",
    "    pass\n",
    "#\n",
    "#------------------------\n",
    "# END OF PROGRAM\n",
    "#-----------------------"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf-gpu",
   "language": "python",
   "name": "tf-gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
